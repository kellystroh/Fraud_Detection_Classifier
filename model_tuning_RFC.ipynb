{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, classification_report\n",
    "\n",
    "### imports for performing Random Search \n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Model\n",
    "\n",
    "## In this notebook, we'll walk through building and tuning an initial random forest classification model. Later, we will improve on this by incorporating additional features extracted with Natural Language Processing (NLP) techniques, as well as utilizing a Bayesian optimizer for model selection and hyperparameter tuning. This model will serve as a baseline for comparing later versions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start by loading the refined dataset\n",
    "\n",
    "data = pd.read_csv(\"data/clean_data.csv\", index_col=0)\n",
    "target = pd.read_csv('data/target.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We want to remove three columns in our loaded dataset, which were included for future use in NLP notebooks. \n",
    "\n",
    "data.drop(['clean_desc', 'lem_tokens','lem_text'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Features\n",
    "\n",
    "Using the python file *process_df*, we have already cleaned up the original data, extracted new features, and removed columns that won't work with our classification model (i.e. non-categorical text fields). \n",
    "\n",
    "Below is a list of the remaining feature names. For now, you can review the meanings of the original columns in the EDA notebook, and the meaning of the engineered columns in the *feature engineering* notebook. Later, I will be adding those explanations in this notebook for easier access. \n",
    "\n",
    "#### Categorical columns in original data: \n",
    "fb_published, analytics, logo, map, member_type, header,channels, listed, payout_method, currency, country, delivery_method\n",
    "#### Engineered categorical columns:\n",
    "desc_has_link, org_has_link, email_suffix_code, country_match\n",
    "#### Continuous columns in original data: \n",
    "body_length, name_length, user_age, org_facebook, org_twitter, sale_duration\n",
    "#### Engineered continuous columns\n",
    "num_tiers, tickets_available, average_ticket_price, avg_previous_payouts, num_previous_payouts, org_desc_len, venue_name_len, org_name_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding\n",
    "As per the lists above, we know several of our columns are categorical. We'll use one-hot encoding to convert these for use in our classification model. Every value within a categorical field will be assigned a binary column, with 1's in every row where that value occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14337, 34)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_one_hot = data.copy()\n",
    "df_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here is a list of the categorical columns we need to convert\n",
    "convert_cols = ['desc_has_link', 'org_has_link', 'email_suffix_code', \n",
    "            'country_match','fb_published', 'analytics', 'logo', 'map', 'member_type', 'header',\n",
    "            'channels', 'listed', 'payout_method', 'currency', 'country', 'delivery_method']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_hot = pd.get_dummies(data = df_one_hot, columns = convert_cols, prefix = convert_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14337, 148)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_one_hot\n",
    "y = target.fraud\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "This is an initial model, and we don't want to spend too much time optimizing parameters, given that we will be adjusting the model components in subsequent steps. That said, we want this model to be a good basis for comparison, which requires choosing reasonably good hyperparameters. Random search is a relatively quick option for optimization, so we'll use that for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'class_weight': ['balanced', None],\n",
      " 'max_depth': [40, 43, 46, 50, 53, 56, 60, 63, 66, 70, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [800, 877, 955, 1033, 1111, 1188, 1266, 1344, 1422, 1500]}\n"
     ]
    }
   ],
   "source": [
    "# n_estimators\n",
    "n_estimators = [int(x) for x in np.linspace(start = 800, stop = 1500, num = 10)]\n",
    "\n",
    "# max_features\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# max_depth\n",
    "max_depth = [int(x) for x in np.linspace(40, 70, num = 10)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# class_weight\n",
    "class_weight = ['balanced', None]\n",
    "\n",
    "# min_samples_split\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# min_samples_leaf\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# bootstrap\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'class_weight': class_weight,\n",
    "               'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators='warn',\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_sc...\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'class_weight': ['balanced', None],\n",
       "                                        'max_depth': [40, 43, 46, 50, 53, 56,\n",
       "                                                      60, 63, 66, 70, None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [800, 877, 955, 1033,\n",
       "                                                         1111, 1188, 1266, 1344,\n",
       "                                                         1422, 1500]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=8, refit=True,\n",
       "                   return_train_score=False, scoring='recall', verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First create the base model to tune\n",
    "rfc = RandomForestClassifier(random_state=8)\n",
    "\n",
    "# Definition of the random search\n",
    "random_search = RandomizedSearchCV(estimator=rfc,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=10,\n",
    "                                   scoring='recall',\n",
    "                                   cv=3, \n",
    "                                   verbose=1, \n",
    "                                   random_state=8)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Random Search are:\n",
      "{'n_estimators': 800, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 66, 'class_weight': 'balanced', 'bootstrap': False}\n",
      "\n",
      "The recall of a model with these hyperparameters is:\n",
      "0.9255219011415805\n"
     ]
    }
   ],
   "source": [
    "print(\"The best hyperparameters from Random Search are:\")\n",
    "print(random_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The recall of a model with these hyperparameters is:\")\n",
    "print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model on the training data & generate predictions for the test data\n",
    "\n",
    "While the Random Search yielded the calculation of best 'recall' score, we want to see the model performance in greater depth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here we'll use the best hyperparameters found above\n",
    "rfc = RandomForestClassifier(n_estimators=800, oob_score=True, random_state=33, class_weight='balanced', \n",
    "                             min_samples_leaf=4, min_samples_split=10, max_depth=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "                       criterion='gini', max_depth=66, max_features='auto',\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=4,\n",
       "                       min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=800, n_jobs=None, oob_score=True,\n",
       "                       random_state=33, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = rfc.predict(X_test)\n",
    "y_hat = pd.Series(y_hat, index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import actual_vs_predicted_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test set includes 2609 non-fraudulent events & 259 fraudulent events.\n",
      "The model predicted 2616 non-fraudulent events & 252 fraudulent events.\n"
     ]
    }
   ],
   "source": [
    "predicted_fraud, predicted_nf, actual_fraud, actual_nf = actual_vs_predicted_counts(y_hat, y_test)\n",
    "\n",
    "print(f'The test set includes {actual_nf} non-fraudulent events & {actual_fraud} fraudulent events.')\n",
    "print(f'The model predicted {predicted_nf} non-fraudulent events & {predicted_fraud} fraudulent events.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True or False?  Positive or Negative?\n",
    "\n",
    "The language of *true positives* and *false negatives* is used to discuss several key measures of the quality of any classification model. If these terms aren't familiar, here's a quick recap: \n",
    "\n",
    "Recall that our 'y' variable indicates whether or not an event is fraudulent. *y_test* contains the actual values of y for each event in the test group, while *y_hat* contains the predictions (fraud or not fraud) that our model made for each event in the test set. \n",
    "\n",
    "A result is labeled 'True' if the model's prediction is accurate (AKA y_test and y_hat are the same for a specific event)\n",
    "A result is labeled 'False' if the model's prediction is incorrect  (AKA y_test and y_hat are the different for a specific event)\n",
    "\n",
    "In this case, positive/negative reflects whether the  model predicted fraud or not fraud. \n",
    "\n",
    "A result is labeled 'Positive' if the model predicts an event is fraudulent\n",
    "A result is labeled 'Negative' if the model predicts an event is NOT fraudulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2599 true negatives\n",
      "There are 17 false negatives\n",
      "There are 242 true positives\n",
      "There are 10 false positives\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_hat).ravel()\n",
    "print(f'There are {tn} true negatives')\n",
    "print(f'There are {fn} false negatives')\n",
    "print(f'There are {tp} true positives')\n",
    "print(f'There are {fp} false positives')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results above tell us that the model made 2826 correct predictions (true positives + true negatives). We also see that the model missed 19 fraudulent events, and falsely suspected 23 real events. These numbers will be used to calculate several 'scores' used to measure & compare model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   not fraud       0.99      1.00      0.99      2609\n",
      "       fraud       0.96      0.93      0.95       259\n",
      "\n",
      "    accuracy                           0.99      2868\n",
      "   macro avg       0.98      0.97      0.97      2868\n",
      "weighted avg       0.99      0.99      0.99      2868\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['not fraud', 'fraud']\n",
    "print(classification_report(y_test, y_hat, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Score : Out of all events predicted to be fraud, what percent is actually fraud? \n",
    "The precision score formula is true positives / (true positives + false positives)\n",
    "\n",
    "### Recall Score : Out of all fraudulent events, what percent were correctly identified by model?\n",
    "The recall score formula is true positives / (true positives + false negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Probabilities\n",
    "\n",
    "So far, we have only looked at the model predictions in terms of 'Fraud' or 'Not Fraud'. The model actually calculates the probability that a given event is fraudulent. If the probability is .5 or above, the prediction is labeled 'Fraud'. Otherwise, it is labeled 'Not Fraud'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The predict_proba method will give us an array with two columns (probability of not fraud, probability of fraud)\n",
    "pred_prob = rfc.predict_proba(X_test)\n",
    "\n",
    "### We will make a pandas series in order to index the predicted probabilities by IDs\n",
    "pred_prob_fraud = pd.Series(pred_prob[:,1], index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, instead of just knowing the counts of true positives, etc., we want to look at what probability the model assigned to each event... Specifically, we are interested in the probabilities for the events that were incorrectly classified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import result_sets\n",
    "tp_set, fp_set, tn_set, fn_set = result_sets(y_hat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11488    0.635326\n",
       "384      0.671959\n",
       "12583    0.511809\n",
       "4365     0.536370\n",
       "1966     0.621044\n",
       "12468    0.723382\n",
       "8343     0.752333\n",
       "506      0.718382\n",
       "8155     0.608939\n",
       "7967     0.894880\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prob_fraud[fp_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12096    0.169328\n",
       "7554     0.033541\n",
       "4510     0.267263\n",
       "8638     0.369047\n",
       "2519     0.266564\n",
       "4104     0.287447\n",
       "11560    0.144238\n",
       "6537     0.232978\n",
       "7082     0.207805\n",
       "11503    0.313629\n",
       "11504    0.074367\n",
       "10515    0.323575\n",
       "5622     0.059058\n",
       "2583     0.070586\n",
       "11163    0.212940\n",
       "9527     0.390345\n",
       "12510    0.101951\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prob_fraud[fn_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, pred_prob[:,1], pos_label=True)\n",
    "plt.plot(fpr,tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = list(np.around(rfc.feature_importances_, 3))\n",
    "feat_list = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'body_length': 0.03,\n",
       " 'name_length': 0.012,\n",
       " 'org_facebook': 0.046,\n",
       " 'org_twitter': 0.023,\n",
       " 'sale_duration': 0.1,\n",
       " 'user_age': 0.069,\n",
       " 'member_created': 0.029,\n",
       " 'num_tiers': 0.006,\n",
       " 'tickets_available': 0.01,\n",
       " 'average_ticket_price': 0.023,\n",
       " 'min_price': 0.021,\n",
       " 'max_price': 0.019,\n",
       " 'potential_revenue': 0.015,\n",
       " 'org_desc_len': 0.008,\n",
       " 'venue_name_len': 0.011,\n",
       " 'org_name_len': 0.027,\n",
       " 'avg_past_payouts': 0.16,\n",
       " 'num_past_payouts': 0.168,\n",
       " 'desc_has_link_False': 0.004,\n",
       " 'desc_has_link_True': 0.006,\n",
       " 'org_has_link_False': 0.001,\n",
       " 'org_has_link_True': 0.002,\n",
       " 'email_suffix_code_0': 0.002,\n",
       " 'email_suffix_code_1': 0.002,\n",
       " 'email_suffix_code_2': 0.003,\n",
       " 'email_suffix_code_3': 0.0,\n",
       " 'email_suffix_code_4': 0.0,\n",
       " 'email_suffix_code_5': 0.0,\n",
       " 'email_suffix_code_6': 0.0,\n",
       " 'email_suffix_code_7': 0.002,\n",
       " 'email_suffix_code_8': 0.0,\n",
       " 'email_suffix_code_9': 0.0,\n",
       " 'country_match_False': 0.009,\n",
       " 'country_match_True': 0.01,\n",
       " 'fb_published_0': 0.002,\n",
       " 'fb_published_1': 0.002,\n",
       " 'analytics_0': 0.001,\n",
       " 'analytics_1': 0.001,\n",
       " 'logo_0': 0.004,\n",
       " 'logo_1': 0.003,\n",
       " 'map_0': 0.003,\n",
       " 'map_1': 0.003,\n",
       " 'member_type_1': 0.035,\n",
       " 'member_type_2': 0.0,\n",
       " 'member_type_3': 0.01,\n",
       " 'member_type_4': 0.004,\n",
       " 'member_type_5': 0.0,\n",
       " 'member_type_103': 0.0,\n",
       " 'header_0.0': 0.002,\n",
       " 'header_1.0': 0.002,\n",
       " 'channels_0': 0.003,\n",
       " 'channels_4': 0.001,\n",
       " 'channels_5': 0.002,\n",
       " 'channels_6': 0.001,\n",
       " 'channels_7': 0.0,\n",
       " 'channels_8': 0.002,\n",
       " 'channels_9': 0.0,\n",
       " 'channels_10': 0.0,\n",
       " 'channels_11': 0.001,\n",
       " 'channels_12': 0.0,\n",
       " 'channels_13': 0.001,\n",
       " 'listed_n': 0.001,\n",
       " 'listed_y': 0.001,\n",
       " 'payout_method_ACH': 0.005,\n",
       " 'payout_method_CHECK': 0.012,\n",
       " 'payout_method_X': 0.032,\n",
       " 'currency_AUD': 0.001,\n",
       " 'currency_CAD': 0.001,\n",
       " 'currency_EUR': 0.0,\n",
       " 'currency_GBP': 0.003,\n",
       " 'currency_MXN': 0.0,\n",
       " 'currency_NZD': 0.0,\n",
       " 'currency_USD': 0.001,\n",
       " 'country_A1': 0.0,\n",
       " 'country_AE': 0.0,\n",
       " 'country_AR': 0.0,\n",
       " 'country_AT': 0.0,\n",
       " 'country_AU': 0.0,\n",
       " 'country_BB': 0.0,\n",
       " 'country_BE': 0.0,\n",
       " 'country_BG': 0.0,\n",
       " 'country_BS': 0.0,\n",
       " 'country_CA': 0.001,\n",
       " 'country_CH': 0.0,\n",
       " 'country_CI': 0.0,\n",
       " 'country_CM': 0.0,\n",
       " 'country_CN': 0.0,\n",
       " 'country_CO': 0.0,\n",
       " 'country_CR': 0.0,\n",
       " 'country_CZ': 0.0,\n",
       " 'country_DE': 0.0,\n",
       " 'country_DK': 0.0,\n",
       " 'country_DZ': 0.0,\n",
       " 'country_EC': 0.0,\n",
       " 'country_ES': 0.0,\n",
       " 'country_FI': 0.0,\n",
       " 'country_FR': 0.0,\n",
       " 'country_GB': 0.003,\n",
       " 'country_GH': 0.0,\n",
       " 'country_GR': 0.0,\n",
       " 'country_HR': 0.0,\n",
       " 'country_HU': 0.0,\n",
       " 'country_ID': 0.0,\n",
       " 'country_IE': 0.0,\n",
       " 'country_IL': 0.0,\n",
       " 'country_IM': 0.0,\n",
       " 'country_IN': 0.0,\n",
       " 'country_IS': 0.0,\n",
       " 'country_IT': 0.0,\n",
       " 'country_JE': 0.0,\n",
       " 'country_JM': 0.0,\n",
       " 'country_KE': 0.0,\n",
       " 'country_KH': 0.0,\n",
       " 'country_LB': 0.0,\n",
       " 'country_MA': 0.0,\n",
       " 'country_MX': 0.0,\n",
       " 'country_MY': 0.0,\n",
       " 'country_NG': 0.0,\n",
       " 'country_NI': 0.0,\n",
       " 'country_NL': 0.0,\n",
       " 'country_NZ': 0.0,\n",
       " 'country_PE': 0.0,\n",
       " 'country_PH': 0.0,\n",
       " 'country_PK': 0.0,\n",
       " 'country_PR': 0.0,\n",
       " 'country_PS': 0.0,\n",
       " 'country_PT': 0.0,\n",
       " 'country_QA': 0.0,\n",
       " 'country_RO': 0.0,\n",
       " 'country_RS': 0.0,\n",
       " 'country_RU': 0.0,\n",
       " 'country_SE': 0.0,\n",
       " 'country_SG': 0.0,\n",
       " 'country_SI': 0.0,\n",
       " 'country_TH': 0.0,\n",
       " 'country_TJ': 0.0,\n",
       " 'country_TR': 0.0,\n",
       " 'country_US': 0.002,\n",
       " 'country_UY': 0.0,\n",
       " 'country_VE': 0.0,\n",
       " 'country_VI': 0.0,\n",
       " 'country_VN': 0.0,\n",
       " 'country_X': 0.0,\n",
       " 'country_ZA': 0.0,\n",
       " 'delivery_method_0.0': 0.016,\n",
       " 'delivery_method_1.0': 0.017,\n",
       " 'delivery_method_3.0': 0.001,\n",
       " 'delivery_method_86.0': 0.001}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_imp_dict = {}\n",
    "for t,r in zip(feat_list, feat_imp):\n",
    "    feat_imp_dict[t] = r\n",
    "feat_imp_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
